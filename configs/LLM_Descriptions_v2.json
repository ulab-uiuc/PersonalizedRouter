{
    "LLaMA-3 (8b)": {
        "feature": "This is a relatively small-sized model (8 billion parameters) designed for general-purpose\nlanguage tasks. Its low cost per million tokens (0.2) makes it an affordable option for many\napplications requiring quick responses with moderate accuracy.",
        "input_price": 0.2,
        "output_price": 0.2,
        "model": "meta-llama/llama-3-8b-instruct"
    },
    "Mixtral-8x7B": {
        "feature": "With a combined size of 56 billion parameters, this model aims to provide stronger language\nmodeling capabilities. Its cost per million tokens is 0.6, reflecting its balance between\nperformance and affordability for more complex tasks.",
        "input_price": 0.6,
        "output_price": 0.6,
        "model": "mistralai/mixtral-8x7b-instruct"
    },
    "NousResearch": {
        "feature": "A mid-sized model with 34 billion parameters, suitable for handling moderately complex\nlanguage tasks. Its cost is higher at 0.8 per million tokens, indicating a greater computational\ndemand, likely due to its enhanced capabilities over smaller models.",
        "input_price": 0.9,
        "output_price": 0.9,
        "model": "nousresearch/nous-hermes-2-mixtral-8x7b-dpo"
    },
    "Ministral-8b": {
        "feature": "A highly efficient model with 8 billion parameters, tailored for fast performance and \noptimized cost-effectiveness. With a cost of just 0.2 per million tokens, it delivers rapid processing\n while maintaining exceptional value for resource usage.",
        "input_price": 0.2,
        "output_price": 0.2,
        "model": "mistralai/ministral-8b"
    },
    "Mistral-7b": {
        "feature": "With 7 billion parameters, Mistral-7b is optimized for lightweight tasks, balancing speed and\nefficiency. Its cost per million tokens is 0.2, making it cost-effective for standard use cases\nwithout the need for complex computations.",
        "input_price": 0.2,
        "output_price": 0.2,
        "model": "mistralai/mistral-7b-instruct-v0.3"
    },
    "LLaMA-2 (70b)": {
        "feature": "A larger variant of LLaMA-2, this model has 70 billion parameters, providing advanced capa-\nbilities for complex tasks. Its cost per million tokens is 0.9, indicating its higher computational\ndemand and enhanced performance.",
        "input_price": 0.9,
        "output_price": 0.9,
        "model": "meta-llama/llama-2-70b-chat"
    },
    "LLaMA-3.1 (8b)": {
        "feature": "A variant optimized for speed and efficiency with 8 billion parameters. Its cost per million\ntokens is only 0.2, suggesting that it is designed to handle tasks quickly while being highly\ncost-effective.",
        "input_price": 0.2,
        "output_price": 0.2,
        "model": "meta-llama/llama-3.1-8b-instruct"
    },
    "LLaMA-3 (70b)": {
        "feature": "This model, at 70 billion parameters, is tailored for high performance with an emphasis on\nefficiency. The cost is 0.9 per million tokens, reflecting its advanced capabilities for a broad\nrange of tasks requiring more computation.",
        "input_price": 0.9,
        "output_price": 0.9,
        "model": "meta-llama/llama-3-70b-instruct"
    },
    "Llama-3.1 (70b)": {
        "feature": "Large model with 70 billion parameters, likely to offer strong capabilities for various language\ntasks. Its cost is also 0.9 per million tokens, suggesting similar performance and computational\nneeds as other 70b models.",
        "input_price": 0.9,
        "output_price": 0.9,
        "model": "meta-llama/llama-3.1-70b-instruct"
    },
    "Qwen-2 (72b)": {
        "feature": "With 72 billion parameters, Qwen-2 is among the largest models in the list, designed for\nhigh-complexity tasks. Its cost per million tokens is 0.9, making it comparable to other\nhigh-performance models in terms of both capability and expense.",
        "input_price": 0.9,
        "output_price": 0.9,
        "model": "qwen/qwen-2-72b-instruct"
    }
}